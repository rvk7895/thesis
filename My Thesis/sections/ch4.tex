\section{Summary of the Findings}
The study we've conducted revealed significant biases in how the algorithm recommends matches, with pronounced disparities affecting users of non-binary genders. These findings underscore a broader issue of algorithmic bias within digital dating environments, reflecting societal norms and potentially reinforcing gender stereotypes.

\subsection{Findings of the Algorithmic Bias}
The study found that Bumble's algorithm tends to replicate existing societal biases, which are then manifested in the match recommendations provided to users. Non-binary users, in particular, experienced a marked reduction in the visibility and accessibility of suitable matches, suggesting that the algorithm is optimized for binary gender preferences and fails to accommodate the diversity of gender identities effectively. This aligns with broader findings in the field that suggest algorithms, while ostensibly neutral, often perpetuate the biases present in their training data or design logic \cite{davidson-etal-2019-racial,Selbst_Boyd_Friedler_Venkatasubramanian_Vertesi_2019}.

\subsection{Impact of User Experience}
The disparities in match recommendations not only influence the user experience but can also impact users' perceptions of the app's inclusivity. Non-binary users reported feeling marginalized by the platform, which could deter continued use or engagement. This finding is particularly concerning given the increasing reliance on digital platforms for social interactions and the potential of these platforms to shape social norms \cite{MacLeod_McArthur_2019}.

\subsection{Comparison with Theoretical Expectations}
The empirical evidence from the study suggests a discrepancy between theoretical expectations of algorithm neutrality and the actual performance of these systems. Despite Bumble's claims of promoting a progressive and inclusive dating environment, the algorithmic practices observed align more closely with traditional gender norms, thus limiting the platform's potential to foster genuinely inclusive interactions. These findings resonate with the concerns raised by researchers regarding the ethical implications of algorithmic decision-making in social platforms \cite{Hutson_Taft_Barocas_Levy_2018}.

\subsection{Broader Societal Implications}
The replication of biases has broader implications for the role of technology in society. As algorithms increasingly influence various aspects of social life, the biases they carry can have far-reaching effects on social equity and personal identity. This underscores the need for more rigorous approaches to algorithm design and a stronger regulatory framework to ensure fairness and inclusivity in digital interactions \cite{Lambrecht_Tucker_2019}.

\section{Algorithmic Bias and Societal Impact}
We explore now the relationship between algorithmic bias in Bumble's match recommendations and its broader societal impacts. We delve into how these biases influence not only individual user experiences but also perpetuate existing social inequalities, shaping the digital and social landscape.

\subsection{Manifestation of Algorithmic Bias}
The research highlights that Bumble's algorithm, though designed to facilitate connections based on user preferences, inadvertently prioritizes profiles that conform to binary gender norms. This bias is evident in the algorithm's tendency to present a disproportionate number of opposite-gender matches to users, irrespective of their stated preferences for non-binary identities. Such a design reflects and perpetuates heteronormative assumptions prevalent in society, limiting the visibility of non-binary and gender-diverse users within the platform. This finding aligns with prior research indicating that many digital platforms inherently encode societal biases, whether through the data they utilize or the design choices made by developers \cite{Noble_2018, Benjamin_2019}.

\subsection{Societal Impact of Biased Algorithms}
The implications of these biases extend beyond the digital realm into broader social contexts. By reinforcing binary gender norms, Bumble's algorithm not only affects individual user experiences but also contributes to a culture that marginalizes non-binary identities. This can perpetuate a cycle of exclusion and discrimination that affects societal perceptions of gender and sexuality. The influence of such algorithms on social norms is profound, as they have the power to shape interactions and relationships in significant ways \cite{Eubanks_2018}.

\subsection{Comparative Analysis with Other Social Technologies}
Drawing comparisons with other technologies, the study situates Bumble within a broader ecosystem of algorithmic decision-making platforms, such as social media and employment algorithms, which have been shown to exhibit similar biases. Like Bumble, these platforms often reflect and amplify existing social inequalities, suggesting a pervasive issue across different sectors. The comparative analysis underscores the need for comprehensive approaches to address algorithmic biases across all digital platforms \cite{ONeil_2016}.

\subsection{Recommendations for Addressing Algorithmic Bias}
To mitigate these issues, the thesis proposes several interventions:
\begin{itemize}
    \item \textbf{Enhancing Algorithmic Transparency:} Encouraging companies like Bumble to disclose more about how their algorithms operate can foster greater accountability and allow for the identification and correction of biases.
    \item \textbf{Inclusive Design Practices:} Designing algorithms with inclusivity as a foundational principle can help ensure that all user identities are equally represented and valued. This includes incorporating diverse datasets and perspectives in the development process to minimize bias \cite{Costanza-Chock_2020}.
    \item \textbf{Regular Auditing and Updates:} Implementing regular audits of algorithmic processes to ensure they remain fair and effective over time, adapting to changes in societal norms and technological advancements.
\end{itemize}

\section{Methodological Reflections}
\subsection{Overview of Research Methods}
The study employed a mixed-methods approach, combining quantitative data analysis with qualitative interviews to provide a holistic view of the algorithmic biases present in Bumble's match recommendations. This approach allowed for a detailed examination of both the numerical trends in user interactions and the personal experiences and perceptions of users, particularly those from non-binary and gender-diverse communities \cite{Kalra_Gupta_Varghese_Rangaswamy_2023}.

\subsection{Strengths of the Methodological Approach}
One of the key strengths of this approach is its ability to cross-validate findings across different data sources and methodologies:

\begin{itemize}
    \item \textbf{Quantitative Analysis:} Provided robust data on user behavior and match patterns, which helped identify systemic biases in the algorithm. 
    \item \textbf{Qualitative Interviews:} Offered deep insights into the subjective experiences of users, enriching the understanding of the data's practical implications on real-life user experiences \cite{Kalra_Gupta_Varghese_Rangaswamy_2023}.
\end{itemize}
This mixed-methods approach is particularly effective in exploring complex social phenomena like algorithmic bias, as it captures both the breadth and depth of the issue \cite{Creswell_2009}

\subsection{Limitations of the Study}
Despite its strengths, the study faced several limitations:

\begin{itemize}
    \item \textbf{Sample Size and Diversity:} While efforts were made to include a diverse range of participants, the relatively small sample size and the focus primarily on one geographical location may limit the generalizability of the findings to other settings or populations.
    \item \textbf{Algorithmic Opacity:} The proprietary nature of Bumbleâ€™s algorithm limited the ability to directly observe how decisions are made within the system, necessitating reliance on external manifestations of its behavior, such as user-reported experiences and match patterns 
\end{itemize}

\section{Implications of Design and Policy}
\subsection{Incorporating Inclusivity from the Ground Up}
The study underscores the necessity of incorporating inclusivity at the earliest stages of algorithm design. Dating apps should consider diverse gender identities and sexual orientations from the outset, ensuring that the algorithm is capable of understanding and adapting to a wide range of user preferences. This could involve:

\begin{itemize}
    \item Designing user interfaces that explicitly accommodate a variety of gender presentations, going beyond the traditional binary options.
    \item Adjusting match algorithms to equally prioritize non-binary and gender-nonconforming users, rather than merely adapting binary algorithms that may not adequately address their needs \cite{Kalra_Gupta_Varghese_Rangaswamy_2023}
\end{itemize}

\subsection{Regular User Feedback Integration}
To ensure that algorithms remain relevant and non-discriminatory over time, it is crucial to integrate regular feedback mechanisms into the app. User feedback can provide real-time insights into how well the algorithm meets diverse needs and highlight areas for improvement. This approach can facilitate continuous learning and adaptation, ensuring that the services evolve in line with user expectations and societal changes \cite{Kalra_Gupta_Varghese_Rangaswamy_2023}.

\subsection{Enhanced Algorithmic Transparency}
Transparency about how algorithms make match recommendations can build trust and allow users to make informed choices about their interactions on the platform. By disclosing the factors that influence match suggestions, apps can empower users to better understand and potentially challenge decisions made by the algorithm. This transparency is also crucial for regulatory oversight and accountability \cite{PASQUALE_2015}.

\subsection{Developing Comprehensive Anti-Discrimination Guidelines}
Policymakers should consider developing comprehensive guidelines that address algorithmic discrimination, specifically in the context of digital platforms like dating apps. These guidelines could mandate:

\begin{itemize}
    \item Regular audits of algorithms to assess bias.
    \item The implementation of corrective measures when discriminatory practices are identified.
    \item Reporting requirements to monitor compliance and effectiveness of anti-bias measures \cite{Eubanks_2018}
\end{itemize}

\subsection{Encouraging Ethical AI Development}
Governments and regulatory bodies could provide incentives for the development of ethical AI technologies. This could include funding for research into inclusive algorithm design, grants for startups committed to ethical practices, and awards for innovations that significantly reduce bias in digital platforms. Such policies would not only foster innovation but also ensure that it proceeds along ethically sound lines \cite{Crawford}



